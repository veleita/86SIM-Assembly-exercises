THEORY
------------------------------------------------------------------------------------------------

1. Von Neumann machines are made up of three elements: the CPU, the memory and the I/O.


2. The purpose of
		  a) the system bus is to connect the different hardware components of the computer with each other by conveying electrical signals (representing values 1 or 0) through physical wires.

		  b) the address bus is to identify a specific memory or I/O location through its address.

		  c) the data bus is to transfer the data stored in the different components of the computer system between each other.

		  e) the control bus is to determine how the CPU communicates with the rest of the system. For example, read or write instructions.


3. The data bus is the one that defines the "size" of the processor.


4. The address bus controls how much memory you can have.


5. The size of the data bus does not limit the maximum value the CPU can process, it only affects the speed at which the data can be transfered to the CPU. Larger data buses will be able to
   carry more data on each memory operation.


6. Some data bus sizes:

Processor		8088 | 8086 | 80286 | 80386 | 80386sx | 80486 | 80586/Pentium
Data lines (bits)	8      16     16      32      16        32      64
Banks of memory*	1	2      2       4       2         4       8


7. Some address bus sizes:

Processor		8088 | 8086 | 80286 | 80386 | 80386sx | 80486 | 80586/Pentium
Address lines (bits)	20     20     24      32      24        32      32
Addressable memory	1 Mb   1 Mb   16 Mb   4 Gb    16 Mb     4 Gb    4 Gb


* 8.


9. When you store a sord in byte addressable memory, the L.O byte is stored in the specified address, while the H.O is stored in the following address. For example, if you store the value 00FFh in memory adress 1000, the 1000th memory cell will contain 00, and the 1001th one will contain FF.
			00 | 01
		1000 |  FF   00

The same goes for a double word, the L.O bytes always goes in the specified memory location. Consider the double word 12345678:

			00 | 01 | 02 | 03 | 
		1000 |  78   56   34   12


10. Memory operations (or memory cycles) needed to read a word:

Word Address //	 | 100 | 101 | 102 | 103 | 104 | 105 |
	    // Processor |				     |
--------------------------------------------------------------
		8088	 | 2	 2     2     2     2     2   |
		80286	 | 1	 2     1     2     1     2   |
		80386	 | 1	 1     1     2     1     1   |


11. Memory operations (or memory cycles) needed to read a double word:

Word Address //	 | 100 | 101 | 102 | 103 | 104 | 105 |
	    // Processor |				     |
--------------------------------------------------------------
		8088	 | 4	 4     4     4     4     4   |
		80286	 | 2	 3     2     3     2     3   |
		80386	 | 1	 2     2     2     1     2   |


12. In order to let the processor fetch data in the least possible number of memory cycles, you should always store double words in memory addresses which are evenly divisible by 4, and words in even addresses. It doesn't really matter where you store one-byte data, as it will always be fetched in a single memory cycle.


13. 65,536 (2^16) of the memory addresses in a PC are reserved for the I/O data, while main memory typically disposes of 1,048,576 (2^20), 16,777,216 (2^24) or 4,294,976,296 (2^32) memory locations. Anyways, I/O data can also be stored in main memory if necessary.


14. The purpose of the system clock is to sync up the different operations performed by the computer.


15. A clock cycle is the time that takes for the system clock to complete a period, switching from a zero signal to a one signal.


16. The clock frequency and the clock period are inversely proportional, the shortest the clock period is, the higher the clock frequency.


17. Clock cycles required to read a byte from memory:

Processor    | 8088 | 8086 | 80486
Clock cycles | 4      4      1


18. The memory access time is the number of clock cycles the system requires to access a memory location; longer memory access times usually result in a lower performance.


19. A wait state is an extra clock cycle that the system uses to give some device time to complete an operation.


20. Minimum wait states for a 80486 with a 80ns RAM depending of the clock frequency:

Clock speed | 20 MHz | 25 MHz | 33 MHz | 50 MHz | 100 MHz |
Clock period| 50 ns  | 40 ns  | 30 ns  | 20 ns  |  10 ns  |
Wait states |  1     |  1     |  2     |  3     |   7     |*

*These are the wait states needed strictly for memory fetching, no buffering or decoding.


21. If your CPU runs at 50 MHz, 20ns RAM probably won’t be fast enough to operate at zero wait states. That's because reading from memory is not the only operation the system needs to perform - though it's the most time consumming. Decoding and buffering operations also take place and need some time to be accomplished.


22. Sub-10ns RAMs do exist and can be built. However, they are only commonly used in supercomputers because they are very expensive, consume a lot of power, generate lots of heat and are definitely not something practical to have at home.


23. The cache memory can save wait states because it reduces the time it takes to find data in memory. That's because it extracts some of the data from the main memory and stores it into a smaller "sub memory" - the cache memory -. The smaller amount of data makes it easier for the processor to go through all of the addresses of the cache memory and find the one it is searching for. The lines of data stored in the cache are the ones which the processor is more likely to look for, if it finds the required data in the cache, it saves it the time of looking through the entire main memory.


24. Spatial locality of reference occurs when the addresses of the data that the system is using are near to each other, while temporal locality refers to the frequence with which the system fetches the same addresses.


25. In the following Pascal code
				while i < 10 do begin
					x := x * i;
					i := i + 1;
				end;
    temporal locality of reference occurs because the same values (i and x) and instructions stored in the same addresses are being read and written over and over, and spatial locallity occurs because these two values (i and x) are declared consecutively, so they have been stored in contiguous spaces of memory, and also because the instructions that the code executes are also the same few ones over and over, and they, too, are stored in consecutive memory locations as the system calls them.


26. Cache memory improves the performance of sections of code exhibiting spatial locality of reference because it is more probable to get a hache hit (that's to say, that the program finds the required data in cache memory and doesn't need to seek out for it in the main memory) if the chunks of data you need to access are near to each other. This is due to the way cache memory stores data from the main memory: whenever a program fetches data from memory, the cache memory stores it along with other addresses around that data, foreseeing that the program will most likely seek for some of those addresses later on.


27. Cache memory won't, nonetheless, save any wait states if the system is running a program which uses very dispersed data, which occurs when the instructions and variables are not repeated along the code, if it contains many jump instructions, or if the data addresses it uses are very distant from each other.


28. Effective (average) number of wait states for
	a) 80% cache hit ratio, 10 WS for memory, 0 WS for cache => 2 WS
	b) 90% cache hit ratio, 7 WS for memory, 0 WS for cache => 0.7 WS
	c) 95% cache hit ratio, 10 WS for memory, 1 WS for cache => 1.45 WS
	d) 50% cache hit ratio, 2 WS for memory, 0 WS for cache => 1 WS


29. The purpose of a two level catching system is to increase the cache hit ratio, thus, it saves wait states and enhances the memory performance.


30. Effective (average) number of wait states for
	a) Primary cache: 80% HR, 0 WS. Secondary cache: 95%, 2 WS. Memory access: 10 WS => 0.48 WS
	a) Primary cache: 50% HR, 0 WS. Secondary cache: 98%, 1 WS. Memory access: 5 WS => 0.54 WS
	a) Primary cache: 95% HR, 1 WS. Secondary cache: 98%, 4 WS. Memory access: 10 WS => 0.206 WS


31. The purpose of the bus interface unit is to control the address and data busses when accessing main memory, and to access data in the cache memory; while the execution unit performs the operations needed to complete each instruction; and the control unit fetches the instructions' opcodes from memory and places them in the decoding register for execution.


32. It always takes more than one clock cycle to execute an instruction because each instruction takes several steps, many of which require memory operations, which, as we have seen, can take some time. For example, a mov instruction that uses a memory operand (for example, the "mov ax, [1000]" instruction) takes the following steps:

	- Fetch the instruction from memory.
	- Decode the instruction.
	- Move the ip (instruction pointer) register to the next byte.
	- Fetch the operand from memory.
	- Store the fetched value into the destination register.
	- Move the ip (instruction pointer) register to the next byte.


33. A prefetch queue reduces the execution time of instructions because it allows to perform some of the instruction steps simultaneously. For example, in the instruction described in the previous question, the BIU can fetch the operand while the instruction is being decoded.


34. A pipeline allows you to seemingly execute one instruction per clock cycle because it executes many instructions simultaneously by means of assuring that the BIU is always accessing memory and performing the steps of the different instructions simultaneously whenever they don't affect to each other (data hazards) and they don't require a same piece of hardware (pipeline stalls). For example, the pipeline scheme for the three following instructions

1:	mov	ax, [0050]
2:	mov	bx, 1000
3:	mov	cx, 1

whould be this

1:	| fetch opcode | decode opcode	  | fetch [0050] value | store into ax    |
		       |+ prefetch operand|
2:		       |  fetch opcode    | decode opcode      | store into bx    |
					  |+ prefetch operand  |
3:					  | fetch opcode       | decode opcode    | store into cx |
							       |+ prefetch operand|

Once the pipeline is full (that is, when many operations are being done at the same time, there comes a time when an instruction is being completed every clock cycle.


35. A hazard is, as we anticipated before, a phenomenom that occurs whenever an instruction is conditionned by the execution of some other which appears previously in the code. With the pipeline system, these two instructions can be executing at the same time, and it may happen that some of the steps of the later instruction execute before the necessary steps of the previous one have had time to conclude.


36. The 8486 processor handles these occurrences by introducing delays (sort of the same logic we saw before with the wait states). The code will run properly, but it will run slowlier.


37. You can avoid hazards by carefully arranging the instructions in your code so the interdependent instructions you may have don't appear one right after the other.


38. Jump instructions affect
				a) the prefetch queue because the bytes that the prefetch queue has read and stored are the ones that follow the current ip address, if the ip address jumps abruptly these bytes are no longer useful and the prefetch queue needs to start seading and storing the contiguous bytes again.

				b) the pipeline because the instructions in the pipeline are the ones that follow the opcode in the current ip address, if the ip address jumps abruptly these are no longer the following instructions in the program, so the instructions after the jump instruction (if the condition of the jump happens to be true) need to be flushed off the pipeline.


39. A pipeline stall is an occurence that may happen whenever two instructions need to use the same bus at the same time, obviously one of the instructions will need to wait until that piece of hardware is avaiable.


40. Aside from reducing wait states, cache memory improves the performance of a pipelined system because it allows the prefetch queue to fill itself from the data cache so it doesn't need to use the BIU, avoiding bus contention.


41. A Harvard Architechture Machine is a computer with two separate spaces of memory, each with its own bus: one for the data and the other for the opcodes.


42. A superscalar CPU speeds up execution by adding more execution units, this allows to have larger pipelines because more operations can be performed at once.


43. There are two main techniques that any programmer should use to write fast programs when using a superscalar CPU: avoiding jump instructions as much as possible and sticking with the sortest instructions whenever possible.


44. An interrupt is an external hardware event that causes the CPU to interrupt the current instruction sequence and call a special interrupt service routine that saves all the registers and flags (so that it doesn’t disturb the computation it interrupts), does whatever operation is necessary to handle the source of the interrupt, it restores the registers and flags, and then it resumes execution of the code it interrupted. Many I/O devices generate an
interrupt whenever they have data available or are able to accept data from the CPU. This improves system performance because it saves the time of having to wait for an input event to resume the CPU's activity.


45. Polled I/O is a type of I/O operation that makes the CPU to be constantly testing a port to see if data is
available.


46. The difference between memory-mapped and I/O mapped I/O is that memory-mapped I/O is the resource they use to communicate with the I/O devices. Memory/mapped I/O uses special memory addresses, while I/O-mapped uses special instructions.


47. We say that DMA is a special case of memory-mapped I/O because it reads and writes from memory, only not through the CPU, but through the peripherical device itself.
